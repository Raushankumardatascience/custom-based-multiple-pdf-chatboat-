# custom-based-multiple-pdf-chatboat-

# ğŸ“„ Medical PDF Chatbot (Llama 3.2 + FAISS + Streamlit)

An AI-powered **Medical PDF Assistant** that allows users to upload medical PDFs and ask questions based **only on the document content**.  
The system uses **Retrieval-Augmented Generation (RAG)** with **Llama 3.2**, **FAISS**, and **HuggingFace embeddings** to provide accurate, context-aware answers along with **source page references**.

---

## ğŸš€ Features

- ğŸ“¤ Upload medical PDF documents
- ğŸ§  Automatic document chunking based on PDF size
- ğŸ” Semantic search using FAISS vector database
- ğŸ¤– Context-aware answers using **Llama 3.2 (Ollama)**
- ğŸ“„ Displays source **page numbers** for transparency
- ğŸ©º Professional medical response tone
- âš¡ Fast local inference (no cloud dependency)

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit  
- **LLM**: Llama 3.2 (via Ollama)  
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`  
- **Vector Store**: FAISS  
- **Framework**: LangChain  
- **Language**: Python  

---

â”œâ”€â”€ app.py
â”œâ”€â”€ vectorstore/
â”‚ â””â”€â”€ db_faiss/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

 Installation & Setup

 1ï¸âƒ£ Clone the Repository
bash
git clone https://github.com/your-username/medical-pdf-chatbot.git
cd medical-pdf-chatbot
2ï¸âƒ£ Create Virtual Environment (Recommended)
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
4ï¸âƒ£ Install & Run Ollama
Download Ollama from: https://ollama.com
Then pull the Llama model:

ollama pull llama3.2
5ï¸âƒ£ Run the Application
streamlit run app.py


## ğŸ“‚ Project Structure

